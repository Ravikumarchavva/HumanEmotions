{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "8b37397e-7c34-47cb-9180-d57e953dab82",
   "metadata": {},
   "outputs": [],
   "source": [
    "import warnings\n",
    "warnings.filterwarnings(action=\"always\")\n",
    "from transformers import TFViTModel,ViTFeatureExtractor\n",
    "import tensorflow as tf\n",
    "from tensorflow.keras.layers import Dense,Flatten"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "d11cb782",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Fast image processor class <class 'transformers.models.vit.image_processing_vit_fast.ViTImageProcessorFast'> is available for this model. Using slow image processor class. To use the fast image processor class set `use_fast=True`.\n",
      "All PyTorch model weights were used when initializing TFViTModel.\n",
      "\n",
      "All the weights of TFViTModel were initialized from the PyTorch model.\n",
      "If your task is similar to the task the model of the checkpoint was trained on, you can already use TFViTModel for predictions without further training.\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "[1, 197, 768]"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from transformers import AutoImageProcessor, TFViTModel\n",
    "from datasets import load_dataset\n",
    "\n",
    "dataset = load_dataset(\"huggingface/cats-image\", trust_remote_code=True)\n",
    "image = dataset[\"test\"][\"image\"][0]\n",
    "\n",
    "image_processor = AutoImageProcessor.from_pretrained(\"google/vit-base-patch16-224-in21k\")\n",
    "model = TFViTModel.from_pretrained(\"google/vit-base-patch16-224-in21k\")\n",
    "\n",
    "inputs = image_processor(image, return_tensors=\"tf\")\n",
    "outputs = model(**inputs)\n",
    "\n",
    "last_hidden_states = outputs.last_hidden_state\n",
    "list(last_hidden_states.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "9dd6e161",
   "metadata": {},
   "outputs": [
    {
     "ename": "ValueError",
     "evalue": "All `outputs` values must be KerasTensors. Received: outputs=[[[ 0.15575482  0.09159808  0.15156874 ... -0.31802228 -0.08588564\n   -0.09017096]\n  [-0.22563276  0.08627439  0.4749633  ... -0.1784016   0.1726173\n    0.1329821 ]\n  [ 0.04386187  0.06794535  0.4197473  ... -0.2575924   0.11916597\n    0.012656  ]\n  ...\n  [-0.01536189 -0.03941542  0.16829513 ... -0.16698721  0.18655181\n    0.10237021]\n  [ 0.024981   -0.03776236  0.20438282 ...  0.05172412  0.1486935\n    0.13208441]\n  [-0.17497909 -0.0252762   0.25212413 ... -0.14736335  0.16266349\n    0.1324638 ]]] including invalid value [[[ 0.15575482  0.09159808  0.15156874 ... -0.31802228 -0.08588564\n   -0.09017096]\n  [-0.22563276  0.08627439  0.4749633  ... -0.1784016   0.1726173\n    0.1329821 ]\n  [ 0.04386187  0.06794535  0.4197473  ... -0.2575924   0.11916597\n    0.012656  ]\n  ...\n  [-0.01536189 -0.03941542  0.16829513 ... -0.16698721  0.18655181\n    0.10237021]\n  [ 0.024981   -0.03776236  0.20438282 ...  0.05172412  0.1486935\n    0.13208441]\n  [-0.17497909 -0.0252762   0.25212413 ... -0.14736335  0.16266349\n    0.1324638 ]]] of type <class 'tensorflow.python.framework.ops.EagerTensor'>",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mValueError\u001b[0m                                Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[22], line 1\u001b[0m\n\u001b[0;32m----> 1\u001b[0m hf_model \u001b[38;5;241m=\u001b[39m \u001b[43mtf\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mkeras\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mModel\u001b[49m\u001b[43m(\u001b[49m\u001b[43minputs\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mtf\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mkeras\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mlayers\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mInput\u001b[49m\u001b[43m(\u001b[49m\u001b[43mshape\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m224\u001b[39;49m\u001b[43m,\u001b[49m\u001b[38;5;241;43m224\u001b[39;49m\u001b[43m,\u001b[49m\u001b[38;5;241;43m3\u001b[39;49m\u001b[43m)\u001b[49m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43moutputs\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mlast_hidden_states\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m      2\u001b[0m hf_model\u001b[38;5;241m.\u001b[39msummary()\n",
      "File \u001b[0;32m/usr/local/lib/python3.11/dist-packages/keras/src/utils/tracking.py:26\u001b[0m, in \u001b[0;36mno_automatic_dependency_tracking.<locals>.wrapper\u001b[0;34m(*args, **kwargs)\u001b[0m\n\u001b[1;32m     23\u001b[0m \u001b[38;5;129m@wraps\u001b[39m(fn)\n\u001b[1;32m     24\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mwrapper\u001b[39m(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs):\n\u001b[1;32m     25\u001b[0m     \u001b[38;5;28;01mwith\u001b[39;00m DotNotTrackScope():\n\u001b[0;32m---> 26\u001b[0m         \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mfn\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m/usr/local/lib/python3.11/dist-packages/keras/src/models/functional.py:126\u001b[0m, in \u001b[0;36mFunctional.__init__\u001b[0;34m(self, inputs, outputs, name, **kwargs)\u001b[0m\n\u001b[1;32m    124\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m x \u001b[38;5;129;01min\u001b[39;00m flat_outputs:\n\u001b[1;32m    125\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28misinstance\u001b[39m(x, backend\u001b[38;5;241m.\u001b[39mKerasTensor):\n\u001b[0;32m--> 126\u001b[0m         \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mValueError\u001b[39;00m(\n\u001b[1;32m    127\u001b[0m             \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mAll `outputs` values must be KerasTensors. Received: \u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m    128\u001b[0m             \u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124moutputs=\u001b[39m\u001b[38;5;132;01m{\u001b[39;00moutputs\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m including invalid value \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mx\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m of \u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m    129\u001b[0m             \u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mtype \u001b[39m\u001b[38;5;132;01m{\u001b[39;00m\u001b[38;5;28mtype\u001b[39m(x)\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m    130\u001b[0m         )\n\u001b[1;32m    132\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28mall\u001b[39m(is_input_keras_tensor(t) \u001b[38;5;28;01mfor\u001b[39;00m t \u001b[38;5;129;01min\u001b[39;00m flat_inputs):\n\u001b[1;32m    133\u001b[0m     inputs, outputs \u001b[38;5;241m=\u001b[39m clone_graph_nodes(inputs, outputs)\n",
      "\u001b[0;31mValueError\u001b[0m: All `outputs` values must be KerasTensors. Received: outputs=[[[ 0.15575482  0.09159808  0.15156874 ... -0.31802228 -0.08588564\n   -0.09017096]\n  [-0.22563276  0.08627439  0.4749633  ... -0.1784016   0.1726173\n    0.1329821 ]\n  [ 0.04386187  0.06794535  0.4197473  ... -0.2575924   0.11916597\n    0.012656  ]\n  ...\n  [-0.01536189 -0.03941542  0.16829513 ... -0.16698721  0.18655181\n    0.10237021]\n  [ 0.024981   -0.03776236  0.20438282 ...  0.05172412  0.1486935\n    0.13208441]\n  [-0.17497909 -0.0252762   0.25212413 ... -0.14736335  0.16266349\n    0.1324638 ]]] including invalid value [[[ 0.15575482  0.09159808  0.15156874 ... -0.31802228 -0.08588564\n   -0.09017096]\n  [-0.22563276  0.08627439  0.4749633  ... -0.1784016   0.1726173\n    0.1329821 ]\n  [ 0.04386187  0.06794535  0.4197473  ... -0.2575924   0.11916597\n    0.012656  ]\n  ...\n  [-0.01536189 -0.03941542  0.16829513 ... -0.16698721  0.18655181\n    0.10237021]\n  [ 0.024981   -0.03776236  0.20438282 ...  0.05172412  0.1486935\n    0.13208441]\n  [-0.17497909 -0.0252762   0.25212413 ... -0.14736335  0.16266349\n    0.1324638 ]]] of type <class 'tensorflow.python.framework.ops.EagerTensor'>"
     ]
    }
   ],
   "source": [
    "hf_model = tf.keras.Model(inputs=tf.keras.layers.Input(shape=(224,224,3)), outputs=last_hidden_states)\n",
    "hf_model.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e49af4af",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.0rc1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
