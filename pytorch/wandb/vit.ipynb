{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Vision transformers"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Connect to wandb"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Failed to detect the name of this notebook, you can set it manually with the WANDB_NOTEBOOK_NAME environment variable to enable code saving.\n",
      "wandb: Using wandb-core as the SDK backend. Please refer to https://wandb.me/wandb-core for more information.\n",
      "wandb: Currently logged in as: ravikumarchavva (ravikumarchavva-org). Use `wandb login --relogin` to force relogin\n",
      "wandb: WARNING If you're specifying your api key in code, ensure this code is not shared publicly.\n",
      "wandb: WARNING Consider setting the WANDB_API_KEY environment variable, or running `wandb login` from the command line.\n",
      "wandb: Appending key for api.wandb.ai to your netrc file: C:\\Users\\chavv\\_netrc\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Logged in successfully.\n"
     ]
    }
   ],
   "source": [
    "# Import the necessary libraries\n",
    "import wandb\n",
    "from dotenv import load_dotenv\n",
    "import os\n",
    "\n",
    "# Load the environment variables from the .env file\n",
    "load_dotenv()\n",
    "\n",
    "# Get the API key from the environment variable\n",
    "api_key = os.getenv(\"WANDB_API_KEY\")\n",
    "\n",
    "# Login to Weights & Biases using the API key\n",
    "try:\n",
    "    wandb.login(key=api_key)\n",
    "    print(\"Logged in successfully.\")\n",
    "except Exception as e:\n",
    "    print(f\"Error during login: {e}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Configurations"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "Tracking run with wandb version 0.18.3"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Run data is saved locally in <code>d:\\github\\HumanEmotions\\pytorch\\wandb\\wandb\\run-20241011_190830-ikcxr0e9</code>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Syncing run <strong><a href='https://wandb.ai/ravikumarchavva-org/transformers-human-emotion-estimation-pytorch/runs/ikcxr0e9' target=\"_blank\">human-emotion-estimation-2</a></strong> to <a href='https://wandb.ai/ravikumarchavva-org/transformers-human-emotion-estimation-pytorch' target=\"_blank\">Weights & Biases</a> (<a href='https://wandb.me/run' target=\"_blank\">docs</a>)<br/>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       " View project at <a href='https://wandb.ai/ravikumarchavva-org/transformers-human-emotion-estimation-pytorch' target=\"_blank\">https://wandb.ai/ravikumarchavva-org/transformers-human-emotion-estimation-pytorch</a>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       " View run at <a href='https://wandb.ai/ravikumarchavva-org/transformers-human-emotion-estimation-pytorch/runs/ikcxr0e9' target=\"_blank\">https://wandb.ai/ravikumarchavva-org/transformers-human-emotion-estimation-pytorch/runs/ikcxr0e9</a>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "# Updated Configuration\n",
    "CONFIGURATION = {\n",
    "    'BATCH_SIZE': 32,\n",
    "    'IM_SIZE': 224,\n",
    "    'N_EPOCHS': 10,\n",
    "    'LEARNING_RATE': 1e-5,\n",
    "    'NUM_CLASSES': 3,\n",
    "}\n",
    "\n",
    "run = wandb.init(\n",
    "    # Set the project where this run will be logged\n",
    "    project=\"transformers-human-emotion-estimation-pytorch\",\n",
    "\n",
    "    # Set the experiment name\n",
    "    name=\"human-emotion-estimation-2\",\n",
    "    # Track hyperparameters and run metadata\n",
    "    config={\n",
    "        \"learning_rate\": CONFIGURATION['LEARNING_RATE'],\n",
    "        \"epochs\": CONFIGURATION['N_EPOCHS'],\n",
    "        \"batch_size\": CONFIGURATION['BATCH_SIZE'],\n",
    "        \"image_size\": CONFIGURATION['IM_SIZE'],\n",
    "        \"num_classes\": CONFIGURATION['NUM_CLASSES'],\n",
    "    },\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Load Dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Number of training samples: 6799\n",
      "Number of testing samples: 2280\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "from torchvision import datasets, transforms\n",
    "\n",
    "TRAIN_DIR = '../../EmotionsDataset/train/'\n",
    "TEST_DIR = '../../EmotionsDataset/test/'\n",
    "CLASS_NAMES = ['angry','happy','sad']\n",
    "\n",
    "# Define the transformations for the training and testing datasets\n",
    "transform = transforms.Compose([\n",
    "    transforms.Resize((CONFIGURATION['IM_SIZE'], CONFIGURATION['IM_SIZE'])),\n",
    "    transforms.ToTensor(),\n",
    "])\n",
    "\n",
    "# Load the training and testing datasets\n",
    "train_dataset = datasets.ImageFolder(root=TRAIN_DIR, transform=transform)\n",
    "test_dataset = datasets.ImageFolder(root=TEST_DIR, transform=transform)\n",
    "\n",
    "# Print the number of samples in each dataset\n",
    "print(f\"Number of training samples: {len(train_dataset)}\")\n",
    "print(f\"Number of testing samples: {len(test_dataset)}\")\n",
    "\n",
    "# Create the dataloaders for the training and testing datasets\n",
    "train_loader = torch.utils.data.DataLoader(train_dataset, batch_size=CONFIGURATION['BATCH_SIZE'], shuffle=True)\n",
    "test_loader = torch.utils.data.DataLoader(test_dataset, batch_size=CONFIGURATION['BATCH_SIZE'], shuffle=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Load transformer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of ViTForImageClassification were not initialized from the model checkpoint at google/vit-base-patch16-224-in21k and are newly initialized: ['classifier.bias', 'classifier.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
      "c:\\Users\\chavv\\anaconda\\envs\\huggingface-torch\\lib\\site-packages\\transformers\\models\\vit\\modeling_vit.py:261: UserWarning: 1Torch was not compiled with flash attention. (Triggered internally at C:\\cb\\pytorch_1000000000000\\work\\aten\\src\\ATen\\native\\transformers\\cuda\\sdp_utils.cpp:555.)\n",
      "  context_layer = torch.nn.functional.scaled_dot_product_attention(\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "==============================================================================================================\n",
       "Layer (type:depth-idx)                                       Output Shape              Param #\n",
       "==============================================================================================================\n",
       "ViTForImageClassification                                    [1, 3]                    --\n",
       "├─ViTModel: 1-1                                              [1, 197, 768]             --\n",
       "│    └─ViTEmbeddings: 2-1                                    [1, 197, 768]             152,064\n",
       "│    │    └─ViTPatchEmbeddings: 3-1                          [1, 196, 768]             590,592\n",
       "│    │    └─Dropout: 3-2                                     [1, 197, 768]             --\n",
       "│    └─ViTEncoder: 2-2                                       [1, 197, 768]             --\n",
       "│    │    └─ModuleList: 3-3                                  --                        85,054,464\n",
       "│    └─LayerNorm: 2-3                                        [1, 197, 768]             1,536\n",
       "├─Linear: 1-2                                                [1, 3]                    2,307\n",
       "==============================================================================================================\n",
       "Total params: 85,800,963\n",
       "Trainable params: 85,800,963\n",
       "Non-trainable params: 0\n",
       "Total mult-adds (M): 200.81\n",
       "==============================================================================================================\n",
       "Input size (MB): 0.60\n",
       "Forward/backward pass size (MB): 162.18\n",
       "Params size (MB): 342.60\n",
       "Estimated Total Size (MB): 505.38\n",
       "=============================================================================================================="
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from transformers import AutoImageProcessor, ViTForImageClassification\n",
    "from torchinfo import summary\n",
    "import torch\n",
    "\n",
    "# Check for GPU availability\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "\n",
    "# Load the image processor and model\n",
    "image_processor = AutoImageProcessor.from_pretrained(\"google/vit-base-patch16-224-in21k\", use_fast=True)\n",
    "model = ViTForImageClassification.from_pretrained(\"google/vit-base-patch16-224-in21k\", num_labels=len(CLASS_NAMES))\n",
    "model.to(device)  # Move model to the appropriate device (GPU/CPU)\n",
    "\n",
    "# View model summary\n",
    "summary(model, input_size=(1, 3, 224, 224))  # Adjust input size based on your model"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Model hyperparameters"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "from torch.optim import AdamW\n",
    "from torch.nn import CrossEntropyLoss\n",
    "\n",
    "# Define optimizer and loss function\n",
    "optimizer = AdamW(model.parameters(), lr=CONFIGURATION['LEARNING_RATE'])\n",
    "criterion = torch.nn.CrossEntropyLoss()  # Use Cross Entropy Loss for multi-class classification"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Training"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\chavv\\anaconda\\envs\\huggingface-torch\\lib\\site-packages\\torch\\optim\\lr_scheduler.py:60: UserWarning: The verbose parameter is deprecated. Please use get_last_lr() to access the learning rate.\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/10\n",
      "Batch 0/213 - Loss: 1.0983\n",
      "Batch 10/213 - Loss: 1.1985\n",
      "Batch 20/213 - Loss: 1.0730\n",
      "Batch 30/213 - Loss: 1.0980\n",
      "Batch 40/213 - Loss: 1.0182\n",
      "Batch 50/213 - Loss: 1.0372\n",
      "Batch 60/213 - Loss: 1.0118\n",
      "Batch 70/213 - Loss: 1.1327\n",
      "Batch 80/213 - Loss: 1.1110\n",
      "Batch 90/213 - Loss: 1.0617\n",
      "Batch 100/213 - Loss: 0.9982\n",
      "Batch 110/213 - Loss: 1.0464\n",
      "Batch 120/213 - Loss: 1.0903\n",
      "Batch 130/213 - Loss: 1.1714\n",
      "Batch 140/213 - Loss: 1.1241\n",
      "Batch 150/213 - Loss: 1.0805\n",
      "Batch 160/213 - Loss: 1.0901\n",
      "Batch 170/213 - Loss: 1.0590\n",
      "Batch 180/213 - Loss: 1.0396\n",
      "Batch 190/213 - Loss: 1.0253\n",
      "Batch 200/213 - Loss: 1.0186\n",
      "Batch 210/213 - Loss: 1.0515\n",
      "Training loss: 1.0646, Accuracy: 0.4423, Top-2 Accuracy: 0.7738\n",
      "Validation loss: 1.0641, Accuracy: 0.4412, Top-2 Accuracy: 0.7732\n",
      "Epoch 2/10\n",
      "Batch 0/213 - Loss: 1.0082\n",
      "Batch 10/213 - Loss: 1.0564\n",
      "Batch 20/213 - Loss: 0.9887\n",
      "Batch 30/213 - Loss: 0.9141\n",
      "Batch 40/213 - Loss: 1.1538\n",
      "Batch 50/213 - Loss: 1.0571\n",
      "Batch 60/213 - Loss: 1.1332\n",
      "Batch 70/213 - Loss: 1.0402\n",
      "Batch 80/213 - Loss: 1.0853\n",
      "Batch 90/213 - Loss: 1.0696\n",
      "Batch 100/213 - Loss: 1.0415\n",
      "Batch 110/213 - Loss: 1.0411\n",
      "Batch 120/213 - Loss: 1.0013\n",
      "Batch 130/213 - Loss: 1.0375\n",
      "Batch 140/213 - Loss: 1.1026\n",
      "Batch 150/213 - Loss: 1.0572\n",
      "Batch 160/213 - Loss: 1.0435\n",
      "Batch 170/213 - Loss: 1.0399\n",
      "Batch 180/213 - Loss: 1.1694\n",
      "Batch 190/213 - Loss: 1.0831\n",
      "Batch 200/213 - Loss: 1.0336\n",
      "Batch 210/213 - Loss: 1.0775\n",
      "Training loss: 1.0629, Accuracy: 0.4440, Top-2 Accuracy: 0.7757\n",
      "Validation loss: 1.0640, Accuracy: 0.4412, Top-2 Accuracy: 0.7732\n",
      "Epoch 3/10\n",
      "Batch 0/213 - Loss: 1.1025\n",
      "Batch 10/213 - Loss: 1.1396\n",
      "Batch 20/213 - Loss: 1.1650\n",
      "Batch 30/213 - Loss: 1.0345\n",
      "Batch 40/213 - Loss: 1.1768\n",
      "Batch 50/213 - Loss: 1.0927\n",
      "Batch 60/213 - Loss: 1.1228\n",
      "Batch 70/213 - Loss: 1.0144\n",
      "Batch 80/213 - Loss: 1.0268\n",
      "Batch 90/213 - Loss: 0.9997\n",
      "Batch 100/213 - Loss: 1.0867\n",
      "Batch 110/213 - Loss: 1.0144\n",
      "Batch 120/213 - Loss: 1.0858\n",
      "Batch 130/213 - Loss: 1.0259\n",
      "Batch 140/213 - Loss: 1.0614\n",
      "Batch 150/213 - Loss: 1.0693\n",
      "Batch 160/213 - Loss: 0.9872\n",
      "Batch 170/213 - Loss: 1.0005\n",
      "Batch 180/213 - Loss: 0.9927\n",
      "Batch 190/213 - Loss: 1.0182\n",
      "Batch 200/213 - Loss: 1.0262\n",
      "Batch 210/213 - Loss: 1.0768\n",
      "Training loss: 1.0631, Accuracy: 0.4440, Top-2 Accuracy: 0.7757\n",
      "Validation loss: 1.0648, Accuracy: 0.4412, Top-2 Accuracy: 0.7732\n",
      "Epoch 4/10\n",
      "Batch 0/213 - Loss: 1.0627\n",
      "Batch 10/213 - Loss: 1.1654\n",
      "Batch 20/213 - Loss: 1.0480\n",
      "Batch 30/213 - Loss: 1.0294\n",
      "Batch 40/213 - Loss: 1.0804\n",
      "Batch 50/213 - Loss: 1.0547\n",
      "Batch 60/213 - Loss: 1.0613\n",
      "Batch 70/213 - Loss: 1.0415\n",
      "Batch 80/213 - Loss: 1.0147\n",
      "Batch 90/213 - Loss: 1.0303\n",
      "Batch 100/213 - Loss: 0.9754\n",
      "Batch 110/213 - Loss: 1.0711\n",
      "Batch 120/213 - Loss: 1.0718\n",
      "Batch 130/213 - Loss: 1.0727\n",
      "Batch 140/213 - Loss: 1.0691\n",
      "Batch 150/213 - Loss: 1.0388\n",
      "Batch 160/213 - Loss: 1.0170\n",
      "Batch 170/213 - Loss: 0.9781\n",
      "Batch 180/213 - Loss: 0.9918\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[6], line 35\u001b[0m\n\u001b[0;32m     32\u001b[0m loss\u001b[38;5;241m.\u001b[39mbackward()\n\u001b[0;32m     33\u001b[0m optimizer\u001b[38;5;241m.\u001b[39mstep()\n\u001b[1;32m---> 35\u001b[0m running_loss \u001b[38;5;241m+\u001b[39m\u001b[38;5;241m=\u001b[39m \u001b[43mloss\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mitem\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m     36\u001b[0m _, predicted \u001b[38;5;241m=\u001b[39m torch\u001b[38;5;241m.\u001b[39mmax(probs, \u001b[38;5;241m1\u001b[39m)\n\u001b[0;32m     37\u001b[0m correct \u001b[38;5;241m+\u001b[39m\u001b[38;5;241m=\u001b[39m (predicted \u001b[38;5;241m==\u001b[39m labels)\u001b[38;5;241m.\u001b[39msum()\u001b[38;5;241m.\u001b[39mitem()\n",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "import torch.nn.functional as F\n",
    "from torch.optim.lr_scheduler import ReduceLROnPlateau\n",
    "\n",
    "# Fine-tuned learning rate scheduler\n",
    "scheduler = ReduceLROnPlateau(\n",
    "    optimizer,\n",
    "    mode='min',\n",
    "    factor=0.5,  # Reduce learning rate by half instead of 10x\n",
    "    patience=3,  # Wait for 3 epochs without improvement before reducing\n",
    "    verbose=True,  # Print messages when learning rate is updated\n",
    "    min_lr=1e-7  # Ensure learning rate doesn't go below this value\n",
    ")\n",
    "\n",
    "# Training loop (add softmax, accuracy, and top-k logging)\n",
    "for epoch in range(CONFIGURATION['N_EPOCHS']):\n",
    "    print(f\"Epoch {epoch + 1}/{CONFIGURATION['N_EPOCHS']}\")\n",
    "    model.train()\n",
    "    running_loss = 0.0\n",
    "    correct = 0\n",
    "    total = 0\n",
    "    topk_correct = 0\n",
    "\n",
    "    for batch_idx, (images, labels) in enumerate(train_loader):\n",
    "        images, labels = images.to(device), labels.to(device)\n",
    "        inputs = image_processor(images, return_tensors=\"pt\").to(device)\n",
    "        outputs = model(**inputs)\n",
    "\n",
    "        probs = F.softmax(outputs.logits, dim=1)\n",
    "        loss = criterion(outputs.logits, labels)\n",
    "\n",
    "        optimizer.zero_grad()\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "\n",
    "        running_loss += loss.item()\n",
    "        _, predicted = torch.max(probs, 1)\n",
    "        correct += (predicted == labels).sum().item()\n",
    "        total += labels.size(0)\n",
    "\n",
    "        # Top-K accuracy (k=2)\n",
    "        topk_probs, topk_preds = torch.topk(probs, k=2, dim=1)\n",
    "        topk_correct += torch.sum(topk_preds.eq(labels.view(-1, 1)).sum(1)).item()\n",
    "\n",
    "        if batch_idx % 10 == 0:\n",
    "            print(f\"Batch {batch_idx}/{len(train_loader)} - Loss: {loss.item():.4f}\")\n",
    "\n",
    "    avg_train_loss = running_loss / len(train_loader)\n",
    "    accuracy = correct / total\n",
    "    topk_accuracy = topk_correct / total\n",
    "    print(f\"Training loss: {avg_train_loss:.4f}, Accuracy: {accuracy:.4f}, Top-2 Accuracy: {topk_accuracy:.4f}\")\n",
    "\n",
    "    # Log training metrics to WandB\n",
    "    wandb.log({\n",
    "        \"epoch\": epoch + 1,\n",
    "        \"train_loss\": avg_train_loss,\n",
    "        \"train_accuracy\": accuracy,\n",
    "        \"train_topk_accuracy\": topk_accuracy,\n",
    "    })\n",
    "\n",
    "    # Validation loop\n",
    "    model.eval()\n",
    "    val_loss = 0.0\n",
    "    correct = 0\n",
    "    total = 0\n",
    "    topk_correct = 0\n",
    "    with torch.no_grad():\n",
    "        for images, labels in test_loader:\n",
    "            images, labels = images.to(device), labels.to(device)\n",
    "            inputs = image_processor(images, return_tensors=\"pt\").to(device)\n",
    "            outputs = model(**inputs)\n",
    "\n",
    "            probs = F.softmax(outputs.logits, dim=1)\n",
    "            loss = criterion(outputs.logits, labels)\n",
    "            val_loss += loss.item()\n",
    "\n",
    "            _, predicted = torch.max(probs, 1)\n",
    "            correct += (predicted == labels).sum().item()\n",
    "            total += labels.size(0)\n",
    "\n",
    "            # Top-K accuracy (k=2)\n",
    "            topk_probs, topk_preds = torch.topk(probs, k=2, dim=1)\n",
    "            topk_correct += torch.sum(topk_preds.eq(labels.view(-1, 1)).sum(1)).item()\n",
    "\n",
    "    avg_val_loss = val_loss / len(test_loader)\n",
    "    val_accuracy = correct / total\n",
    "    val_topk_accuracy = topk_correct / total\n",
    "    print(f\"Validation loss: {avg_val_loss:.4f}, Accuracy: {val_accuracy:.4f}, Top-2 Accuracy: {val_topk_accuracy:.4f}\")\n",
    "\n",
    "    # Log validation metrics to WandB\n",
    "    wandb.log({\n",
    "        \"epoch\": epoch + 1,\n",
    "        \"val_loss\": avg_val_loss,\n",
    "        \"val_accuracy\": val_accuracy,\n",
    "        \"val_topk_accuracy\": val_topk_accuracy,\n",
    "    })\n",
    "\n",
    "    # Learning rate scheduler step (based on validation loss)\n",
    "    scheduler.step(avg_val_loss)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy: 0.4412\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\chavv\\anaconda\\envs\\huggingface-torch\\lib\\site-packages\\sklearn\\metrics\\_classification.py:1531: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.\n",
      "  _warn_prf(average, modifier, f\"{metric.capitalize()} is\", len(result))\n",
      "c:\\Users\\chavv\\anaconda\\envs\\huggingface-torch\\lib\\site-packages\\sklearn\\metrics\\_classification.py:1531: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.\n",
      "  _warn_prf(average, modifier, f\"{metric.capitalize()} is\", len(result))\n",
      "c:\\Users\\chavv\\anaconda\\envs\\huggingface-torch\\lib\\site-packages\\sklearn\\metrics\\_classification.py:1531: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.\n",
      "  _warn_prf(average, modifier, f\"{metric.capitalize()} is\", len(result))\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Classification Report:\n",
      "               precision    recall  f1-score   support\n",
      "\n",
      "       angry       0.00      0.00      0.00       517\n",
      "       happy       0.44      1.00      0.61      1006\n",
      "         sad       0.00      0.00      0.00       757\n",
      "\n",
      "    accuracy                           0.44      2280\n",
      "   macro avg       0.15      0.33      0.20      2280\n",
      "weighted avg       0.19      0.44      0.27      2280\n",
      "\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "from sklearn.metrics import classification_report, accuracy_score\n",
    "\n",
    "# Load the pre-trained model\n",
    "model_path = 'vit-emotion-classification'\n",
    "model = ViTForImageClassification.from_pretrained(model_path, num_labels=len(CLASS_NAMES))\n",
    "\n",
    "# Move the model to the appropriate device\n",
    "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
    "model.to(device)\n",
    "\n",
    "# Set the model to evaluation mode\n",
    "model.eval()\n",
    "\n",
    "# Initialize lists to store true labels and predictions\n",
    "true_labels = []\n",
    "predictions = []\n",
    "\n",
    "# Disable gradient calculation for inference\n",
    "with torch.no_grad():\n",
    "    for images, labels in test_loader:\n",
    "        # Move images and labels to the device\n",
    "        images, labels = images.to(device), labels.to(device)\n",
    "\n",
    "        # Preprocess images using the image processor\n",
    "        inputs = image_processor(images, return_tensors=\"pt\").to(device)\n",
    "\n",
    "        # Forward pass\n",
    "        outputs = model(**inputs)\n",
    "        _, preds = torch.max(outputs.logits, 1)\n",
    "\n",
    "        # Store true labels and predictions\n",
    "        true_labels.extend(labels.cpu().numpy())\n",
    "        predictions.extend(preds.cpu().numpy())\n",
    "\n",
    "# Calculate accuracy\n",
    "accuracy = accuracy_score(true_labels, predictions)\n",
    "print(f\"Accuracy: {accuracy:.4f}\")\n",
    "\n",
    "# Generate classification report\n",
    "report = classification_report(true_labels, predictions, target_names=CLASS_NAMES)\n",
    "print(\"Classification Report:\\n\", report)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Collecting torchmetrics\n",
      "  Downloading torchmetrics-1.4.3-py3-none-any.whl.metadata (19 kB)\n",
      "Requirement already satisfied: numpy>1.20.0 in c:\\users\\chavv\\anaconda\\envs\\huggingface-torch\\lib\\site-packages (from torchmetrics) (1.26.4)\n",
      "Requirement already satisfied: packaging>17.1 in c:\\users\\chavv\\anaconda\\envs\\huggingface-torch\\lib\\site-packages (from torchmetrics) (24.1)\n",
      "Requirement already satisfied: torch>=1.10.0 in c:\\users\\chavv\\anaconda\\envs\\huggingface-torch\\lib\\site-packages (from torchmetrics) (2.4.1)\n",
      "Collecting lightning-utilities>=0.8.0 (from torchmetrics)\n",
      "  Downloading lightning_utilities-0.11.7-py3-none-any.whl.metadata (5.2 kB)\n",
      "Requirement already satisfied: setuptools in c:\\users\\chavv\\anaconda\\envs\\huggingface-torch\\lib\\site-packages (from lightning-utilities>=0.8.0->torchmetrics) (75.1.0)\n",
      "Requirement already satisfied: typing-extensions in c:\\users\\chavv\\anaconda\\envs\\huggingface-torch\\lib\\site-packages (from lightning-utilities>=0.8.0->torchmetrics) (4.11.0)\n",
      "Requirement already satisfied: filelock in c:\\users\\chavv\\anaconda\\envs\\huggingface-torch\\lib\\site-packages (from torch>=1.10.0->torchmetrics) (3.13.1)\n",
      "Requirement already satisfied: sympy in c:\\users\\chavv\\anaconda\\envs\\huggingface-torch\\lib\\site-packages (from torch>=1.10.0->torchmetrics) (1.13.2)\n",
      "Requirement already satisfied: networkx in c:\\users\\chavv\\anaconda\\envs\\huggingface-torch\\lib\\site-packages (from torch>=1.10.0->torchmetrics) (3.2.1)\n",
      "Requirement already satisfied: jinja2 in c:\\users\\chavv\\anaconda\\envs\\huggingface-torch\\lib\\site-packages (from torch>=1.10.0->torchmetrics) (3.1.4)\n",
      "Requirement already satisfied: fsspec in c:\\users\\chavv\\anaconda\\envs\\huggingface-torch\\lib\\site-packages (from torch>=1.10.0->torchmetrics) (2024.6.1)\n",
      "Requirement already satisfied: MarkupSafe>=2.0 in c:\\users\\chavv\\anaconda\\envs\\huggingface-torch\\lib\\site-packages (from jinja2->torch>=1.10.0->torchmetrics) (2.1.3)\n",
      "Requirement already satisfied: mpmath<1.4,>=1.1.0 in c:\\users\\chavv\\anaconda\\envs\\huggingface-torch\\lib\\site-packages (from sympy->torch>=1.10.0->torchmetrics) (1.3.0)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Downloading torchmetrics-1.4.3-py3-none-any.whl (869 kB)\n",
      "   ---------------------------------------- 0.0/869.5 kB ? eta -:--:--\n",
      "   ------------------------ --------------- 524.3/869.5 kB 4.2 MB/s eta 0:00:01\n",
      "   ---------------------------------------- 869.5/869.5 kB 2.6 MB/s eta 0:00:00\n",
      "Downloading lightning_utilities-0.11.7-py3-none-any.whl (26 kB)\n",
      "Installing collected packages: lightning-utilities, torchmetrics\n",
      "Successfully installed lightning-utilities-0.11.7 torchmetrics-1.4.3\n"
     ]
    }
   ],
   "source": [
    "!pip install torchmetrics\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "huggingface-torch",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.20"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
