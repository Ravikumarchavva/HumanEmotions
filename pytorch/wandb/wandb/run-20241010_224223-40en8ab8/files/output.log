Number of training samples: 6799
Number of testing samples: 2280
torch.Size([3, 224, 224])
Collecting transformers
  Downloading transformers-4.45.2-py3-none-any.whl.metadata (44 kB)
Requirement already satisfied: filelock in c:\users\chavv\anaconda\envs\huggingface-torch\lib\site-packages (from transformers) (3.13.1)
Collecting huggingface-hub<1.0,>=0.23.2 (from transformers)
  Downloading huggingface_hub-0.25.2-py3-none-any.whl.metadata (13 kB)
Requirement already satisfied: numpy>=1.17 in c:\users\chavv\anaconda\envs\huggingface-torch\lib\site-packages (from transformers) (2.0.1)
Requirement already satisfied: packaging>=20.0 in c:\users\chavv\anaconda\envs\huggingface-torch\lib\site-packages (from transformers) (24.1)
Requirement already satisfied: pyyaml>=5.1 in c:\users\chavv\anaconda\envs\huggingface-torch\lib\site-packages (from transformers) (6.0.1)
Collecting regex!=2019.12.17 (from transformers)
  Downloading regex-2024.9.11-cp39-cp39-win_amd64.whl.metadata (41 kB)
Requirement already satisfied: requests in c:\users\chavv\anaconda\envs\huggingface-torch\lib\site-packages (from transformers) (2.32.3)
Collecting safetensors>=0.4.1 (from transformers)
  Downloading safetensors-0.4.5-cp39-none-win_amd64.whl.metadata (3.9 kB)
Collecting tokenizers<0.21,>=0.20 (from transformers)
  Downloading tokenizers-0.20.1-cp39-none-win_amd64.whl.metadata (6.9 kB)
Collecting tqdm>=4.27 (from transformers)
  Downloading tqdm-4.66.5-py3-none-any.whl.metadata (57 kB)
Collecting fsspec>=2023.5.0 (from huggingface-hub<1.0,>=0.23.2->transformers)
  Downloading fsspec-2024.9.0-py3-none-any.whl.metadata (11 kB)
Requirement already satisfied: typing-extensions>=3.7.4.3 in c:\users\chavv\anaconda\envs\huggingface-torch\lib\site-packages (from huggingface-hub<1.0,>=0.23.2->transformers) (4.11.0)
Requirement already satisfied: colorama in c:\users\chavv\anaconda\envs\huggingface-torch\lib\site-packages (from tqdm>=4.27->transformers) (0.4.6)
Requirement already satisfied: charset-normalizer<4,>=2 in c:\users\chavv\anaconda\envs\huggingface-torch\lib\site-packages (from requests->transformers) (3.3.2)
Requirement already satisfied: idna<4,>=2.5 in c:\users\chavv\anaconda\envs\huggingface-torch\lib\site-packages (from requests->transformers) (3.7)
Requirement already satisfied: urllib3<3,>=1.21.1 in c:\users\chavv\anaconda\envs\huggingface-torch\lib\site-packages (from requests->transformers) (2.2.3)
Requirement already satisfied: certifi>=2017.4.17 in c:\users\chavv\anaconda\envs\huggingface-torch\lib\site-packages (from requests->transformers) (2024.8.30)
Downloading transformers-4.45.2-py3-none-any.whl (9.9 MB)
   ---------------------------------------- 0.0/9.9 MB ? eta -:--:--
   -- ------------------------------------- 0.5/9.9 MB 8.2 MB/s eta 0:00:02
   ---- ----------------------------------- 1.0/9.9 MB 2.3 MB/s eta 0:00:04
   ------- -------------------------------- 1.8/9.9 MB 2.8 MB/s eta 0:00:03
   --------- ------------------------------ 2.4/9.9 MB 2.7 MB/s eta 0:00:03
   ----------- ---------------------------- 2.9/9.9 MB 2.7 MB/s eta 0:00:03
   ------------ --------------------------- 3.1/9.9 MB 2.6 MB/s eta 0:00:03
   --------------- ------------------------ 3.9/9.9 MB 2.6 MB/s eta 0:00:03
   ------------------ --------------------- 4.5/9.9 MB 2.6 MB/s eta 0:00:03
   -------------------- ------------------- 5.0/9.9 MB 2.6 MB/s eta 0:00:02
   ---------------------- ----------------- 5.5/9.9 MB 2.6 MB/s eta 0:00:02
   ------------------------ --------------- 6.0/9.9 MB 2.6 MB/s eta 0:00:02
   ------------------------- -------------- 6.3/9.9 MB 2.6 MB/s eta 0:00:02
   ---------------------------- ----------- 7.1/9.9 MB 2.6 MB/s eta 0:00:02
   ----------------------------- ---------- 7.3/9.9 MB 2.5 MB/s eta 0:00:02
   -------------------------------- ------- 8.1/9.9 MB 2.5 MB/s eta 0:00:01
   ----------------------------------- ---- 8.7/9.9 MB 2.5 MB/s eta 0:00:01
   ------------------------------------- -- 9.2/9.9 MB 2.5 MB/s eta 0:00:01
   ---------------------------------------  9.7/9.9 MB 2.5 MB/s eta 0:00:01
   ---------------------------------------- 9.9/9.9 MB 2.5 MB/s eta 0:00:00
Downloading huggingface_hub-0.25.2-py3-none-any.whl (436 kB)
Downloading regex-2024.9.11-cp39-cp39-win_amd64.whl (274 kB)
Downloading safetensors-0.4.5-cp39-none-win_amd64.whl (286 kB)
Downloading tokenizers-0.20.1-cp39-none-win_amd64.whl (2.4 MB)
   ---------------------------------------- 0.0/2.4 MB ? eta -:--:--
   -------- ------------------------------- 0.5/2.4 MB 2.8 MB/s eta 0:00:01
   ----------------- ---------------------- 1.0/2.4 MB 3.1 MB/s eta 0:00:01
   -------------------------- ------------- 1.6/2.4 MB 2.5 MB/s eta 0:00:01
   ---------------------------------------  2.4/2.4 MB 2.7 MB/s eta 0:00:01
   ---------------------------------------- 2.4/2.4 MB 2.5 MB/s eta 0:00:00
Downloading tqdm-4.66.5-py3-none-any.whl (78 kB)
Downloading fsspec-2024.9.0-py3-none-any.whl (179 kB)
Installing collected packages: tqdm, safetensors, regex, fsspec, huggingface-hub, tokenizers, transformers
Successfully installed fsspec-2024.9.0 huggingface-hub-0.25.2 regex-2024.9.11 safetensors-0.4.5 tokenizers-0.20.1 tqdm-4.66.5 transformers-4.45.2
Number of training samples: 6799
Number of testing samples: 2280
c:\Users\chavv\anaconda\envs\huggingface-torch\lib\site-packages\tqdm\auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html
  from .autonotebook import tqdm as notebook_tqdm
c:\Users\chavv\anaconda\envs\huggingface-torch\lib\site-packages\huggingface_hub\file_download.py:147: UserWarning: `huggingface_hub` cache-system uses symlinks by default to efficiently store duplicated files but your machine does not support them in C:\Users\chavv\.cache\huggingface\hub\models--google--vit-base-patch16-224-in21k. Caching files will still work but in a degraded version that might require more space on your disk. This warning can be disabled by setting the `HF_HUB_DISABLE_SYMLINKS_WARNING` environment variable. For more details, see https://huggingface.co/docs/huggingface_hub/how-to-cache#limitations.
To support symlinks on Windows, you either need to activate Developer Mode or to run Python as an administrator. In order to activate developer mode, see this article: https://docs.microsoft.com/en-us/windows/apps/get-started/enable-your-device-for-development
  warnings.warn(message)
c:\Users\chavv\anaconda\envs\huggingface-torch\lib\site-packages\transformers\models\vit\feature_extraction_vit.py:28: FutureWarning: The class ViTFeatureExtractor is deprecated and will be removed in version 5 of Transformers. Please use ViTImageProcessor instead.
  warnings.warn(
c:\Users\chavv\anaconda\envs\huggingface-torch\lib\site-packages\huggingface_hub\file_download.py:147: UserWarning: `huggingface_hub` cache-system uses symlinks by default to efficiently store duplicated files but your machine does not support them in C:\Users\chavv\.cache\huggingface\hub\models--google--vit-base-patch16-224. Caching files will still work but in a degraded version that might require more space on your disk. This warning can be disabled by setting the `HF_HUB_DISABLE_SYMLINKS_WARNING` environment variable. For more details, see https://huggingface.co/docs/huggingface_hub/how-to-cache#limitations.
To support symlinks on Windows, you either need to activate Developer Mode or to run Python as an administrator. In order to activate developer mode, see this article: https://docs.microsoft.com/en-us/windows/apps/get-started/enable-your-device-for-development
  warnings.warn(message)
