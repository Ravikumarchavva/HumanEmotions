Number of training samples: 6799
Number of testing samples: 2280
Some weights of ViTForImageClassification were not initialized from the model checkpoint at google/vit-base-patch16-224-in21k and are newly initialized: ['classifier.bias', 'classifier.weight']
You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.
c:\Users\chavv\anaconda\envs\huggingface-torch\lib\site-packages\transformers\models\vit\modeling_vit.py:261: UserWarning: 1Torch was not compiled with flash attention. (Triggered internally at C:\cb\pytorch_1000000000000\work\aten\src\ATen\native\transformers\cuda\sdp_utils.cpp:555.)
  context_layer = torch.nn.functional.scaled_dot_product_attention(
c:\Users\chavv\anaconda\envs\huggingface-torch\lib\site-packages\torch\optim\lr_scheduler.py:60: UserWarning: The verbose parameter is deprecated. Please use get_last_lr() to access the learning rate.
  warnings.warn(
Epoch 1/10
Batch 0/213 - Loss: 1.0983
Batch 10/213 - Loss: 1.1985
Batch 20/213 - Loss: 1.0730
Batch 30/213 - Loss: 1.0980
Batch 40/213 - Loss: 1.0182
Batch 50/213 - Loss: 1.0372
Batch 60/213 - Loss: 1.0118
Batch 70/213 - Loss: 1.1327
Batch 80/213 - Loss: 1.1110
Batch 90/213 - Loss: 1.0617
Batch 100/213 - Loss: 0.9982
Batch 110/213 - Loss: 1.0464
Batch 120/213 - Loss: 1.0903
Batch 130/213 - Loss: 1.1714
Batch 140/213 - Loss: 1.1241
Batch 150/213 - Loss: 1.0805
Batch 160/213 - Loss: 1.0901
Batch 170/213 - Loss: 1.0590
Batch 180/213 - Loss: 1.0396
Batch 190/213 - Loss: 1.0253
Batch 200/213 - Loss: 1.0186
Batch 210/213 - Loss: 1.0515
Training loss: 1.0646, Accuracy: 0.4423, Top-2 Accuracy: 0.7738
Validation loss: 1.0641, Accuracy: 0.4412, Top-2 Accuracy: 0.7732
Epoch 2/10
Batch 0/213 - Loss: 1.0082
Batch 10/213 - Loss: 1.0564
Batch 20/213 - Loss: 0.9887
Batch 30/213 - Loss: 0.9141
Batch 40/213 - Loss: 1.1538
Batch 50/213 - Loss: 1.0571
Batch 60/213 - Loss: 1.1332
Batch 70/213 - Loss: 1.0402
Batch 80/213 - Loss: 1.0853
Batch 90/213 - Loss: 1.0696
Batch 100/213 - Loss: 1.0415
Batch 110/213 - Loss: 1.0411
Batch 120/213 - Loss: 1.0013
Batch 130/213 - Loss: 1.0375
Batch 140/213 - Loss: 1.1026
Batch 150/213 - Loss: 1.0572
Batch 160/213 - Loss: 1.0435
Batch 170/213 - Loss: 1.0399
Batch 180/213 - Loss: 1.1694
Batch 190/213 - Loss: 1.0831
Batch 200/213 - Loss: 1.0336
Batch 210/213 - Loss: 1.0775
Training loss: 1.0629, Accuracy: 0.4440, Top-2 Accuracy: 0.7757
Validation loss: 1.0640, Accuracy: 0.4412, Top-2 Accuracy: 0.7732
Epoch 3/10
Batch 0/213 - Loss: 1.1025
Batch 10/213 - Loss: 1.1396
Batch 20/213 - Loss: 1.1650
Batch 30/213 - Loss: 1.0345
Batch 40/213 - Loss: 1.1768
Batch 50/213 - Loss: 1.0927
Batch 60/213 - Loss: 1.1228
Batch 70/213 - Loss: 1.0144
Batch 80/213 - Loss: 1.0268
Batch 90/213 - Loss: 0.9997
Batch 100/213 - Loss: 1.0867
Batch 110/213 - Loss: 1.0144
Batch 120/213 - Loss: 1.0858
Batch 130/213 - Loss: 1.0259
Batch 140/213 - Loss: 1.0614
Batch 150/213 - Loss: 1.0693
Batch 160/213 - Loss: 0.9872
Batch 170/213 - Loss: 1.0005
Batch 180/213 - Loss: 0.9927
Batch 190/213 - Loss: 1.0182
Batch 200/213 - Loss: 1.0262
Batch 210/213 - Loss: 1.0768
Training loss: 1.0631, Accuracy: 0.4440, Top-2 Accuracy: 0.7757
Validation loss: 1.0648, Accuracy: 0.4412, Top-2 Accuracy: 0.7732
Epoch 4/10
Batch 0/213 - Loss: 1.0627
Batch 10/213 - Loss: 1.1654
Batch 20/213 - Loss: 1.0480
Batch 30/213 - Loss: 1.0294
Batch 40/213 - Loss: 1.0804
Batch 50/213 - Loss: 1.0547
Batch 60/213 - Loss: 1.0613
Batch 70/213 - Loss: 1.0415
Batch 80/213 - Loss: 1.0147
Batch 90/213 - Loss: 1.0303
Batch 100/213 - Loss: 0.9754
Batch 110/213 - Loss: 1.0711
Batch 120/213 - Loss: 1.0718
Batch 130/213 - Loss: 1.0727
Batch 140/213 - Loss: 1.0691
Batch 150/213 - Loss: 1.0388
Batch 160/213 - Loss: 1.0170
Batch 170/213 - Loss: 0.9781
Batch 180/213 - Loss: 0.9918
