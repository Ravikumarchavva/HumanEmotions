Number of training samples: 6799
Number of testing samples: 2280
Some weights of ViTForImageClassification were not initialized from the model checkpoint at google/vit-base-patch16-224-in21k and are newly initialized: ['classifier.bias', 'classifier.weight']
You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.
c:\Users\chavv\anaconda\envs\huggingface-torch\lib\site-packages\transformers\models\vit\modeling_vit.py:261: UserWarning: 1Torch was not compiled with flash attention. (Triggered internally at C:\cb\pytorch_1000000000000\work\aten\src\ATen\native\transformers\cuda\sdp_utils.cpp:555.)
  context_layer = torch.nn.functional.scaled_dot_product_attention(
c:\Users\chavv\anaconda\envs\huggingface-torch\lib\site-packages\torch\optim\lr_scheduler.py:60: UserWarning: The verbose parameter is deprecated. Please use get_last_lr() to access the learning rate.
  warnings.warn(
Epoch 1/10
Batch 0/213 - Loss: 1.1143
Batch 10/213 - Loss: 1.0770
Batch 20/213 - Loss: 1.0114
Batch 30/213 - Loss: 1.0339
Batch 40/213 - Loss: 1.1314
Batch 50/213 - Loss: 1.0342
Batch 60/213 - Loss: 0.9790
Batch 70/213 - Loss: 1.0924
Batch 80/213 - Loss: 1.1328
Batch 90/213 - Loss: 1.0716
Batch 100/213 - Loss: 1.1047
Batch 110/213 - Loss: 1.0729
Batch 120/213 - Loss: 1.0768
Batch 130/213 - Loss: 1.0789
Batch 140/213 - Loss: 1.0827
Batch 150/213 - Loss: 1.0656
Batch 160/213 - Loss: 1.0647
Batch 170/213 - Loss: 1.0448
Batch 180/213 - Loss: 0.9821
Batch 190/213 - Loss: 1.0641
Batch 200/213 - Loss: 1.0099
Batch 210/213 - Loss: 1.1109
Training loss: 1.0653
Validation loss: 1.0644
Epoch 2/10
Batch 0/213 - Loss: 1.0848
Batch 10/213 - Loss: 1.0524
Batch 20/213 - Loss: 0.9616
Batch 30/213 - Loss: 1.0963
Batch 40/213 - Loss: 1.1290
Batch 50/213 - Loss: 1.0960
Batch 60/213 - Loss: 1.1043
Batch 70/213 - Loss: 1.1409
Batch 80/213 - Loss: 1.1172
Batch 90/213 - Loss: 1.1431
Batch 100/213 - Loss: 1.1328
Batch 110/213 - Loss: 1.1121
Batch 120/213 - Loss: 1.0794
Batch 130/213 - Loss: 1.1226
Batch 140/213 - Loss: 1.0270
Batch 150/213 - Loss: 1.0195
Batch 160/213 - Loss: 1.0028
Batch 170/213 - Loss: 1.1777
Batch 180/213 - Loss: 1.0615
Batch 190/213 - Loss: 1.0487
Batch 200/213 - Loss: 1.0613
Batch 210/213 - Loss: 1.0598
Training loss: 1.0629
Validation loss: 1.0641
Epoch 3/10
Batch 0/213 - Loss: 1.0931
Batch 10/213 - Loss: 1.0565
wandb: WARNING Calling wandb.login() after wandb.init() has no effect.
Logged in successfully.
