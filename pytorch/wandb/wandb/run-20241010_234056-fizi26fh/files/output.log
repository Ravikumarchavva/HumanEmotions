Number of training samples: 6799
Number of testing samples: 2280
Some weights of ViTForImageClassification were not initialized from the model checkpoint at google/vit-base-patch16-224-in21k and are newly initialized: ['classifier.bias', 'classifier.weight']
You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.
Collecting torchsummary
  Downloading torchsummary-1.5.1-py3-none-any.whl.metadata (296 bytes)
Downloading torchsummary-1.5.1-py3-none-any.whl (2.8 kB)
Installing collected packages: torchsummary
Successfully installed torchsummary-1.5.1
c:\Users\chavv\anaconda\envs\huggingface-torch\lib\site-packages\transformers\models\vit\modeling_vit.py:261: UserWarning: 1Torch was not compiled with flash attention. (Triggered internally at C:\cb\pytorch_1000000000000\work\aten\src\ATen\native\transformers\cuda\sdp_utils.cpp:555.)
  context_layer = torch.nn.functional.scaled_dot_product_attention(
ViTForImageClassification(
  (vit): ViTModel(
    (embeddings): ViTEmbeddings(
      (patch_embeddings): ViTPatchEmbeddings(
        (projection): Conv2d(3, 768, kernel_size=(16, 16), stride=(16, 16))
      )
      (dropout): Dropout(p=0.0, inplace=False)
    )
    (encoder): ViTEncoder(
      (layer): ModuleList(
        (0-11): 12 x ViTLayer(
          (attention): ViTSdpaAttention(
            (attention): ViTSdpaSelfAttention(
              (query): Linear(in_features=768, out_features=768, bias=True)
              (key): Linear(in_features=768, out_features=768, bias=True)
              (value): Linear(in_features=768, out_features=768, bias=True)
              (dropout): Dropout(p=0.0, inplace=False)
            )
            (output): ViTSelfOutput(
              (dense): Linear(in_features=768, out_features=768, bias=True)
              (dropout): Dropout(p=0.0, inplace=False)
            )
          )
          (intermediate): ViTIntermediate(
            (dense): Linear(in_features=768, out_features=3072, bias=True)
            (intermediate_act_fn): GELUActivation()
          )
          (output): ViTOutput(
            (dense): Linear(in_features=3072, out_features=768, bias=True)
            (dropout): Dropout(p=0.0, inplace=False)
          )
          (layernorm_before): LayerNorm((768,), eps=1e-12, elementwise_affine=True)
          (layernorm_after): LayerNorm((768,), eps=1e-12, elementwise_affine=True)
        )
      )
    )
    (layernorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)
  )
  (classifier): Linear(in_features=768, out_features=3, bias=True)
)
Collecting torchinfo
  Downloading torchinfo-1.8.0-py3-none-any.whl.metadata (21 kB)
Downloading torchinfo-1.8.0-py3-none-any.whl (23 kB)
Installing collected packages: torchinfo
Successfully installed torchinfo-1.8.0
Layer: vit.embeddings.cls_token | Size: torch.Size([1, 1, 768]) | Requires Grad: True
Layer: vit.embeddings.position_embeddings | Size: torch.Size([1, 197, 768]) | Requires Grad: True
Layer: vit.embeddings.patch_embeddings.projection.weight | Size: torch.Size([768, 3, 16, 16]) | Requires Grad: True
Layer: vit.embeddings.patch_embeddings.projection.bias | Size: torch.Size([768]) | Requires Grad: True
Layer: vit.encoder.layer.0.attention.attention.query.weight | Size: torch.Size([768, 768]) | Requires Grad: True
Layer: vit.encoder.layer.0.attention.attention.query.bias | Size: torch.Size([768]) | Requires Grad: True
Layer: vit.encoder.layer.0.attention.attention.key.weight | Size: torch.Size([768, 768]) | Requires Grad: True
Layer: vit.encoder.layer.0.attention.attention.key.bias | Size: torch.Size([768]) | Requires Grad: True
Layer: vit.encoder.layer.0.attention.attention.value.weight | Size: torch.Size([768, 768]) | Requires Grad: True
Layer: vit.encoder.layer.0.attention.attention.value.bias | Size: torch.Size([768]) | Requires Grad: True
Layer: vit.encoder.layer.0.attention.output.dense.weight | Size: torch.Size([768, 768]) | Requires Grad: True
Layer: vit.encoder.layer.0.attention.output.dense.bias | Size: torch.Size([768]) | Requires Grad: True
Layer: vit.encoder.layer.0.intermediate.dense.weight | Size: torch.Size([3072, 768]) | Requires Grad: True
Layer: vit.encoder.layer.0.intermediate.dense.bias | Size: torch.Size([3072]) | Requires Grad: True
Layer: vit.encoder.layer.0.output.dense.weight | Size: torch.Size([768, 3072]) | Requires Grad: True
Layer: vit.encoder.layer.0.output.dense.bias | Size: torch.Size([768]) | Requires Grad: True
Layer: vit.encoder.layer.0.layernorm_before.weight | Size: torch.Size([768]) | Requires Grad: True
Layer: vit.encoder.layer.0.layernorm_before.bias | Size: torch.Size([768]) | Requires Grad: True
Layer: vit.encoder.layer.0.layernorm_after.weight | Size: torch.Size([768]) | Requires Grad: True
Layer: vit.encoder.layer.0.layernorm_after.bias | Size: torch.Size([768]) | Requires Grad: True
Layer: vit.encoder.layer.1.attention.attention.query.weight | Size: torch.Size([768, 768]) | Requires Grad: True
Layer: vit.encoder.layer.1.attention.attention.query.bias | Size: torch.Size([768]) | Requires Grad: True
Layer: vit.encoder.layer.1.attention.attention.key.weight | Size: torch.Size([768, 768]) | Requires Grad: True
Layer: vit.encoder.layer.1.attention.attention.key.bias | Size: torch.Size([768]) | Requires Grad: True
Layer: vit.encoder.layer.1.attention.attention.value.weight | Size: torch.Size([768, 768]) | Requires Grad: True
Layer: vit.encoder.layer.1.attention.attention.value.bias | Size: torch.Size([768]) | Requires Grad: True
Layer: vit.encoder.layer.1.attention.output.dense.weight | Size: torch.Size([768, 768]) | Requires Grad: True
Layer: vit.encoder.layer.1.attention.output.dense.bias | Size: torch.Size([768]) | Requires Grad: True
Layer: vit.encoder.layer.1.intermediate.dense.weight | Size: torch.Size([3072, 768]) | Requires Grad: True
Layer: vit.encoder.layer.1.intermediate.dense.bias | Size: torch.Size([3072]) | Requires Grad: True
Layer: vit.encoder.layer.1.output.dense.weight | Size: torch.Size([768, 3072]) | Requires Grad: True
Layer: vit.encoder.layer.1.output.dense.bias | Size: torch.Size([768]) | Requires Grad: True
Layer: vit.encoder.layer.1.layernorm_before.weight | Size: torch.Size([768]) | Requires Grad: True
Layer: vit.encoder.layer.1.layernorm_before.bias | Size: torch.Size([768]) | Requires Grad: True
Layer: vit.encoder.layer.1.layernorm_after.weight | Size: torch.Size([768]) | Requires Grad: True
Layer: vit.encoder.layer.1.layernorm_after.bias | Size: torch.Size([768]) | Requires Grad: True
Layer: vit.encoder.layer.2.attention.attention.query.weight | Size: torch.Size([768, 768]) | Requires Grad: True
Layer: vit.encoder.layer.2.attention.attention.query.bias | Size: torch.Size([768]) | Requires Grad: True
Layer: vit.encoder.layer.2.attention.attention.key.weight | Size: torch.Size([768, 768]) | Requires Grad: True
Layer: vit.encoder.layer.2.attention.attention.key.bias | Size: torch.Size([768]) | Requires Grad: True
Layer: vit.encoder.layer.2.attention.attention.value.weight | Size: torch.Size([768, 768]) | Requires Grad: True
Layer: vit.encoder.layer.2.attention.attention.value.bias | Size: torch.Size([768]) | Requires Grad: True
Layer: vit.encoder.layer.2.attention.output.dense.weight | Size: torch.Size([768, 768]) | Requires Grad: True
Layer: vit.encoder.layer.2.attention.output.dense.bias | Size: torch.Size([768]) | Requires Grad: True
Layer: vit.encoder.layer.2.intermediate.dense.weight | Size: torch.Size([3072, 768]) | Requires Grad: True
Layer: vit.encoder.layer.2.intermediate.dense.bias | Size: torch.Size([3072]) | Requires Grad: True
Layer: vit.encoder.layer.2.output.dense.weight | Size: torch.Size([768, 3072]) | Requires Grad: True
Layer: vit.encoder.layer.2.output.dense.bias | Size: torch.Size([768]) | Requires Grad: True
Layer: vit.encoder.layer.2.layernorm_before.weight | Size: torch.Size([768]) | Requires Grad: True
Layer: vit.encoder.layer.2.layernorm_before.bias | Size: torch.Size([768]) | Requires Grad: True
Layer: vit.encoder.layer.2.layernorm_after.weight | Size: torch.Size([768]) | Requires Grad: True
Layer: vit.encoder.layer.2.layernorm_after.bias | Size: torch.Size([768]) | Requires Grad: True
Layer: vit.encoder.layer.3.attention.attention.query.weight | Size: torch.Size([768, 768]) | Requires Grad: True
Layer: vit.encoder.layer.3.attention.attention.query.bias | Size: torch.Size([768]) | Requires Grad: True
Layer: vit.encoder.layer.3.attention.attention.key.weight | Size: torch.Size([768, 768]) | Requires Grad: True
Layer: vit.encoder.layer.3.attention.attention.key.bias | Size: torch.Size([768]) | Requires Grad: True
Layer: vit.encoder.layer.3.attention.attention.value.weight | Size: torch.Size([768, 768]) | Requires Grad: True
Layer: vit.encoder.layer.3.attention.attention.value.bias | Size: torch.Size([768]) | Requires Grad: True
Layer: vit.encoder.layer.3.attention.output.dense.weight | Size: torch.Size([768, 768]) | Requires Grad: True
Layer: vit.encoder.layer.3.attention.output.dense.bias | Size: torch.Size([768]) | Requires Grad: True
Layer: vit.encoder.layer.3.intermediate.dense.weight | Size: torch.Size([3072, 768]) | Requires Grad: True
Layer: vit.encoder.layer.3.intermediate.dense.bias | Size: torch.Size([3072]) | Requires Grad: True
Layer: vit.encoder.layer.3.output.dense.weight | Size: torch.Size([768, 3072]) | Requires Grad: True
Layer: vit.encoder.layer.3.output.dense.bias | Size: torch.Size([768]) | Requires Grad: True
Layer: vit.encoder.layer.3.layernorm_before.weight | Size: torch.Size([768]) | Requires Grad: True
Layer: vit.encoder.layer.3.layernorm_before.bias | Size: torch.Size([768]) | Requires Grad: True
Layer: vit.encoder.layer.3.layernorm_after.weight | Size: torch.Size([768]) | Requires Grad: True
Layer: vit.encoder.layer.3.layernorm_after.bias | Size: torch.Size([768]) | Requires Grad: True
Layer: vit.encoder.layer.4.attention.attention.query.weight | Size: torch.Size([768, 768]) | Requires Grad: True
Layer: vit.encoder.layer.4.attention.attention.query.bias | Size: torch.Size([768]) | Requires Grad: True
Layer: vit.encoder.layer.4.attention.attention.key.weight | Size: torch.Size([768, 768]) | Requires Grad: True
Layer: vit.encoder.layer.4.attention.attention.key.bias | Size: torch.Size([768]) | Requires Grad: True
Layer: vit.encoder.layer.4.attention.attention.value.weight | Size: torch.Size([768, 768]) | Requires Grad: True
Layer: vit.encoder.layer.4.attention.attention.value.bias | Size: torch.Size([768]) | Requires Grad: True
Layer: vit.encoder.layer.4.attention.output.dense.weight | Size: torch.Size([768, 768]) | Requires Grad: True
Layer: vit.encoder.layer.4.attention.output.dense.bias | Size: torch.Size([768]) | Requires Grad: True
Layer: vit.encoder.layer.4.intermediate.dense.weight | Size: torch.Size([3072, 768]) | Requires Grad: True
Layer: vit.encoder.layer.4.intermediate.dense.bias | Size: torch.Size([3072]) | Requires Grad: True
Layer: vit.encoder.layer.4.output.dense.weight | Size: torch.Size([768, 3072]) | Requires Grad: True
Layer: vit.encoder.layer.4.output.dense.bias | Size: torch.Size([768]) | Requires Grad: True
Layer: vit.encoder.layer.4.layernorm_before.weight | Size: torch.Size([768]) | Requires Grad: True
Layer: vit.encoder.layer.4.layernorm_before.bias | Size: torch.Size([768]) | Requires Grad: True
Layer: vit.encoder.layer.4.layernorm_after.weight | Size: torch.Size([768]) | Requires Grad: True
Layer: vit.encoder.layer.4.layernorm_after.bias | Size: torch.Size([768]) | Requires Grad: True
Layer: vit.encoder.layer.5.attention.attention.query.weight | Size: torch.Size([768, 768]) | Requires Grad: True
Layer: vit.encoder.layer.5.attention.attention.query.bias | Size: torch.Size([768]) | Requires Grad: True
Layer: vit.encoder.layer.5.attention.attention.key.weight | Size: torch.Size([768, 768]) | Requires Grad: True
Layer: vit.encoder.layer.5.attention.attention.key.bias | Size: torch.Size([768]) | Requires Grad: True
Layer: vit.encoder.layer.5.attention.attention.value.weight | Size: torch.Size([768, 768]) | Requires Grad: True
Layer: vit.encoder.layer.5.attention.attention.value.bias | Size: torch.Size([768]) | Requires Grad: True
Layer: vit.encoder.layer.5.attention.output.dense.weight | Size: torch.Size([768, 768]) | Requires Grad: True
Layer: vit.encoder.layer.5.attention.output.dense.bias | Size: torch.Size([768]) | Requires Grad: True
Layer: vit.encoder.layer.5.intermediate.dense.weight | Size: torch.Size([3072, 768]) | Requires Grad: True
Layer: vit.encoder.layer.5.intermediate.dense.bias | Size: torch.Size([3072]) | Requires Grad: True
Layer: vit.encoder.layer.5.output.dense.weight | Size: torch.Size([768, 3072]) | Requires Grad: True
Layer: vit.encoder.layer.5.output.dense.bias | Size: torch.Size([768]) | Requires Grad: True
Layer: vit.encoder.layer.5.layernorm_before.weight | Size: torch.Size([768]) | Requires Grad: True
Layer: vit.encoder.layer.5.layernorm_before.bias | Size: torch.Size([768]) | Requires Grad: True
Layer: vit.encoder.layer.5.layernorm_after.weight | Size: torch.Size([768]) | Requires Grad: True
Layer: vit.encoder.layer.5.layernorm_after.bias | Size: torch.Size([768]) | Requires Grad: True
Layer: vit.encoder.layer.6.attention.attention.query.weight | Size: torch.Size([768, 768]) | Requires Grad: True
Layer: vit.encoder.layer.6.attention.attention.query.bias | Size: torch.Size([768]) | Requires Grad: True
Layer: vit.encoder.layer.6.attention.attention.key.weight | Size: torch.Size([768, 768]) | Requires Grad: True
Layer: vit.encoder.layer.6.attention.attention.key.bias | Size: torch.Size([768]) | Requires Grad: True
Layer: vit.encoder.layer.6.attention.attention.value.weight | Size: torch.Size([768, 768]) | Requires Grad: True
Layer: vit.encoder.layer.6.attention.attention.value.bias | Size: torch.Size([768]) | Requires Grad: True
Layer: vit.encoder.layer.6.attention.output.dense.weight | Size: torch.Size([768, 768]) | Requires Grad: True
Layer: vit.encoder.layer.6.attention.output.dense.bias | Size: torch.Size([768]) | Requires Grad: True
Layer: vit.encoder.layer.6.intermediate.dense.weight | Size: torch.Size([3072, 768]) | Requires Grad: True
Layer: vit.encoder.layer.6.intermediate.dense.bias | Size: torch.Size([3072]) | Requires Grad: True
Layer: vit.encoder.layer.6.output.dense.weight | Size: torch.Size([768, 3072]) | Requires Grad: True
Layer: vit.encoder.layer.6.output.dense.bias | Size: torch.Size([768]) | Requires Grad: True
Layer: vit.encoder.layer.6.layernorm_before.weight | Size: torch.Size([768]) | Requires Grad: True
Layer: vit.encoder.layer.6.layernorm_before.bias | Size: torch.Size([768]) | Requires Grad: True
Layer: vit.encoder.layer.6.layernorm_after.weight | Size: torch.Size([768]) | Requires Grad: True
Layer: vit.encoder.layer.6.layernorm_after.bias | Size: torch.Size([768]) | Requires Grad: True
Layer: vit.encoder.layer.7.attention.attention.query.weight | Size: torch.Size([768, 768]) | Requires Grad: True
Layer: vit.encoder.layer.7.attention.attention.query.bias | Size: torch.Size([768]) | Requires Grad: True
Layer: vit.encoder.layer.7.attention.attention.key.weight | Size: torch.Size([768, 768]) | Requires Grad: True
Layer: vit.encoder.layer.7.attention.attention.key.bias | Size: torch.Size([768]) | Requires Grad: True
Layer: vit.encoder.layer.7.attention.attention.value.weight | Size: torch.Size([768, 768]) | Requires Grad: True
Layer: vit.encoder.layer.7.attention.attention.value.bias | Size: torch.Size([768]) | Requires Grad: True
Layer: vit.encoder.layer.7.attention.output.dense.weight | Size: torch.Size([768, 768]) | Requires Grad: True
Layer: vit.encoder.layer.7.attention.output.dense.bias | Size: torch.Size([768]) | Requires Grad: True
Layer: vit.encoder.layer.7.intermediate.dense.weight | Size: torch.Size([3072, 768]) | Requires Grad: True
Layer: vit.encoder.layer.7.intermediate.dense.bias | Size: torch.Size([3072]) | Requires Grad: True
Layer: vit.encoder.layer.7.output.dense.weight | Size: torch.Size([768, 3072]) | Requires Grad: True
Layer: vit.encoder.layer.7.output.dense.bias | Size: torch.Size([768]) | Requires Grad: True
Layer: vit.encoder.layer.7.layernorm_before.weight | Size: torch.Size([768]) | Requires Grad: True
Layer: vit.encoder.layer.7.layernorm_before.bias | Size: torch.Size([768]) | Requires Grad: True
Layer: vit.encoder.layer.7.layernorm_after.weight | Size: torch.Size([768]) | Requires Grad: True
Layer: vit.encoder.layer.7.layernorm_after.bias | Size: torch.Size([768]) | Requires Grad: True
Layer: vit.encoder.layer.8.attention.attention.query.weight | Size: torch.Size([768, 768]) | Requires Grad: True
Layer: vit.encoder.layer.8.attention.attention.query.bias | Size: torch.Size([768]) | Requires Grad: True
Layer: vit.encoder.layer.8.attention.attention.key.weight | Size: torch.Size([768, 768]) | Requires Grad: True
Layer: vit.encoder.layer.8.attention.attention.key.bias | Size: torch.Size([768]) | Requires Grad: True
Layer: vit.encoder.layer.8.attention.attention.value.weight | Size: torch.Size([768, 768]) | Requires Grad: True
Layer: vit.encoder.layer.8.attention.attention.value.bias | Size: torch.Size([768]) | Requires Grad: True
Layer: vit.encoder.layer.8.attention.output.dense.weight | Size: torch.Size([768, 768]) | Requires Grad: True
Layer: vit.encoder.layer.8.attention.output.dense.bias | Size: torch.Size([768]) | Requires Grad: True
Layer: vit.encoder.layer.8.intermediate.dense.weight | Size: torch.Size([3072, 768]) | Requires Grad: True
Layer: vit.encoder.layer.8.intermediate.dense.bias | Size: torch.Size([3072]) | Requires Grad: True
Layer: vit.encoder.layer.8.output.dense.weight | Size: torch.Size([768, 3072]) | Requires Grad: True
Layer: vit.encoder.layer.8.output.dense.bias | Size: torch.Size([768]) | Requires Grad: True
Layer: vit.encoder.layer.8.layernorm_before.weight | Size: torch.Size([768]) | Requires Grad: True
Layer: vit.encoder.layer.8.layernorm_before.bias | Size: torch.Size([768]) | Requires Grad: True
Layer: vit.encoder.layer.8.layernorm_after.weight | Size: torch.Size([768]) | Requires Grad: True
Layer: vit.encoder.layer.8.layernorm_after.bias | Size: torch.Size([768]) | Requires Grad: True
Layer: vit.encoder.layer.9.attention.attention.query.weight | Size: torch.Size([768, 768]) | Requires Grad: True
Layer: vit.encoder.layer.9.attention.attention.query.bias | Size: torch.Size([768]) | Requires Grad: True
Layer: vit.encoder.layer.9.attention.attention.key.weight | Size: torch.Size([768, 768]) | Requires Grad: True
Layer: vit.encoder.layer.9.attention.attention.key.bias | Size: torch.Size([768]) | Requires Grad: True
Layer: vit.encoder.layer.9.attention.attention.value.weight | Size: torch.Size([768, 768]) | Requires Grad: True
Layer: vit.encoder.layer.9.attention.attention.value.bias | Size: torch.Size([768]) | Requires Grad: True
Layer: vit.encoder.layer.9.attention.output.dense.weight | Size: torch.Size([768, 768]) | Requires Grad: True
Layer: vit.encoder.layer.9.attention.output.dense.bias | Size: torch.Size([768]) | Requires Grad: True
Layer: vit.encoder.layer.9.intermediate.dense.weight | Size: torch.Size([3072, 768]) | Requires Grad: True
Layer: vit.encoder.layer.9.intermediate.dense.bias | Size: torch.Size([3072]) | Requires Grad: True
Layer: vit.encoder.layer.9.output.dense.weight | Size: torch.Size([768, 3072]) | Requires Grad: True
Layer: vit.encoder.layer.9.output.dense.bias | Size: torch.Size([768]) | Requires Grad: True
Layer: vit.encoder.layer.9.layernorm_before.weight | Size: torch.Size([768]) | Requires Grad: True
Layer: vit.encoder.layer.9.layernorm_before.bias | Size: torch.Size([768]) | Requires Grad: True
Layer: vit.encoder.layer.9.layernorm_after.weight | Size: torch.Size([768]) | Requires Grad: True
Layer: vit.encoder.layer.9.layernorm_after.bias | Size: torch.Size([768]) | Requires Grad: True
Layer: vit.encoder.layer.10.attention.attention.query.weight | Size: torch.Size([768, 768]) | Requires Grad: True
Layer: vit.encoder.layer.10.attention.attention.query.bias | Size: torch.Size([768]) | Requires Grad: True
Layer: vit.encoder.layer.10.attention.attention.key.weight | Size: torch.Size([768, 768]) | Requires Grad: True
Layer: vit.encoder.layer.10.attention.attention.key.bias | Size: torch.Size([768]) | Requires Grad: True
Layer: vit.encoder.layer.10.attention.attention.value.weight | Size: torch.Size([768, 768]) | Requires Grad: True
Layer: vit.encoder.layer.10.attention.attention.value.bias | Size: torch.Size([768]) | Requires Grad: True
Layer: vit.encoder.layer.10.attention.output.dense.weight | Size: torch.Size([768, 768]) | Requires Grad: True
Layer: vit.encoder.layer.10.attention.output.dense.bias | Size: torch.Size([768]) | Requires Grad: True
Layer: vit.encoder.layer.10.intermediate.dense.weight | Size: torch.Size([3072, 768]) | Requires Grad: True
Layer: vit.encoder.layer.10.intermediate.dense.bias | Size: torch.Size([3072]) | Requires Grad: True
Layer: vit.encoder.layer.10.output.dense.weight | Size: torch.Size([768, 3072]) | Requires Grad: True
Layer: vit.encoder.layer.10.output.dense.bias | Size: torch.Size([768]) | Requires Grad: True
Layer: vit.encoder.layer.10.layernorm_before.weight | Size: torch.Size([768]) | Requires Grad: True
Layer: vit.encoder.layer.10.layernorm_before.bias | Size: torch.Size([768]) | Requires Grad: True
Layer: vit.encoder.layer.10.layernorm_after.weight | Size: torch.Size([768]) | Requires Grad: True
Layer: vit.encoder.layer.10.layernorm_after.bias | Size: torch.Size([768]) | Requires Grad: True
Layer: vit.encoder.layer.11.attention.attention.query.weight | Size: torch.Size([768, 768]) | Requires Grad: True
Layer: vit.encoder.layer.11.attention.attention.query.bias | Size: torch.Size([768]) | Requires Grad: True
Layer: vit.encoder.layer.11.attention.attention.key.weight | Size: torch.Size([768, 768]) | Requires Grad: True
Layer: vit.encoder.layer.11.attention.attention.key.bias | Size: torch.Size([768]) | Requires Grad: True
Layer: vit.encoder.layer.11.attention.attention.value.weight | Size: torch.Size([768, 768]) | Requires Grad: True
Layer: vit.encoder.layer.11.attention.attention.value.bias | Size: torch.Size([768]) | Requires Grad: True
Layer: vit.encoder.layer.11.attention.output.dense.weight | Size: torch.Size([768, 768]) | Requires Grad: True
Layer: vit.encoder.layer.11.attention.output.dense.bias | Size: torch.Size([768]) | Requires Grad: True
Layer: vit.encoder.layer.11.intermediate.dense.weight | Size: torch.Size([3072, 768]) | Requires Grad: True
Layer: vit.encoder.layer.11.intermediate.dense.bias | Size: torch.Size([3072]) | Requires Grad: True
Layer: vit.encoder.layer.11.output.dense.weight | Size: torch.Size([768, 3072]) | Requires Grad: True
Layer: vit.encoder.layer.11.output.dense.bias | Size: torch.Size([768]) | Requires Grad: True
Layer: vit.encoder.layer.11.layernorm_before.weight | Size: torch.Size([768]) | Requires Grad: True
Layer: vit.encoder.layer.11.layernorm_before.bias | Size: torch.Size([768]) | Requires Grad: True
Layer: vit.encoder.layer.11.layernorm_after.weight | Size: torch.Size([768]) | Requires Grad: True
Layer: vit.encoder.layer.11.layernorm_after.bias | Size: torch.Size([768]) | Requires Grad: True
Layer: vit.layernorm.weight | Size: torch.Size([768]) | Requires Grad: True
Layer: vit.layernorm.bias | Size: torch.Size([768]) | Requires Grad: True
Layer: classifier.weight | Size: torch.Size([3, 768]) | Requires Grad: True
Layer: classifier.bias | Size: torch.Size([3]) | Requires Grad: True
Some weights of ViTForImageClassification were not initialized from the model checkpoint at google/vit-base-patch16-224-in21k and are newly initialized: ['classifier.bias', 'classifier.weight']
You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.
